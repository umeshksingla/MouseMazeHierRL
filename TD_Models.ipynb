{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Models\n",
    "\n",
    "TD-learning family models:\n",
    "1. <a href='#oneStepTD'> TD(0) </a>\n",
    "2. Actor-critic:\n",
    "    * Some Theory: <a href='http://incompleteideas.net/book/first/ebook/node66.html'>Actor-Critic Methods</a>\n",
    "    * Possible Implementation: <a href='https://www.nature.com/articles/s41598-017-18004-7'> A hippocampo-cerebellar centred network for the learning and execution of sequence-based navigation </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib import cm\n",
    "from copy import deepcopy\n",
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import curve_fit\n",
    "from dataclasses import make_dataclass\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import sys\n",
    "\n",
    "module_path = 'src' \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# Markus's code\n",
    "from MM_Plot_Utils import plot, hist\n",
    "from MM_Maze_Utils import *\n",
    "from MM_Traj_Utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Defining global variables\n",
    "\n",
    "# Some lists of nicknames for mice\n",
    "RewNames=['B1','B2','B3','B4','C1','C3','C6','C7','C8','C9']\n",
    "UnrewNames=['B5','B6','B7','D3','D4','D5','D6','D7','D8','D9']\n",
    "AllNames=RewNames+UnrewNames\n",
    "UnrewNamesSub=['B5','B6','B7','D3','D4','D5','D7','D8','D9'] # excluding D6 which barely entered the maze\n",
    "\n",
    "# Define cell numbers of end/leaf nodes\n",
    "lv6_nodes = list(range(63,127))\n",
    "lv5_nodes = list(range(31,63))\n",
    "lv4_nodes = list(range(15,31))\n",
    "lv3_nodes = list(range(7,15))\n",
    "lv2_nodes = list(range(3,7))\n",
    "lv1_nodes = list(range(1,3))\n",
    "lv0_nodes = list(range(0,1))\n",
    "lvl_dict = {0:lv0_nodes, 1:lv1_nodes, 2:lv2_nodes, 3:lv3_nodes, 4:lv4_nodes, 5:lv5_nodes, 6:lv6_nodes}\n",
    "quad1 = [3,7,8,15,16,17,18,31,32,33,34,35,36,37,38,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78]\n",
    "quad2 = [4,9,10,19,20,21,22,39,40,41,42,43,44,45,46,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94]\n",
    "quad3 = [5,11,12,23,24,25,26]\n",
    "quad3.extend(list(range(47,54)))\n",
    "quad3.extend(list(range(95,110)))\n",
    "quad4 = [6,13,14,27,28,29,30]\n",
    "quad4.extend(list(range(55,62)))\n",
    "quad4.extend(list(range(111,126)))\n",
    "\n",
    "# Parameters for simulating new trajectories\n",
    "InvalidState = -1\n",
    "RewardNode = 116\n",
    "HomeNode = 127\n",
    "StartNode = 0\n",
    "S = 128  # Number of states\n",
    "A = 3\n",
    "RewardNodeMag = 1\n",
    "main_dir = 'stan/'\n",
    "real_traj_dir = main_dir+'traj_data/real_traj/'\n",
    "pred_traj_dir = main_dir+'traj_data/pred_traj/'\n",
    "stan_results_dir = main_dir+'stan_results/'\n",
    "nodemap = pickle.load(open('stan/nodemap.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     21,
     67
    ]
   },
   "outputs": [],
   "source": [
    "def extract_rew_traj(traj_type):\n",
    "    ''' \n",
    "    Segments of real rewarded mice trajectories are extracted and saved. This data will be used for model fitting.\n",
    "    \n",
    "    traj_type: 'first_Rvisit', Storing trajectories up until the first reward node visit (assume immediate reward)\n",
    "                               and discarding all subsequent bouts\n",
    "               'post_firstR_every_Rvisit', Storing trajectories after the first rewarded bout up to the end of the experiment\n",
    "                                           each trajectory terminated at the first reward node visit for the bout\n",
    "               'first_drink', Storing trajectories up until the first reward receipt (could be multiple reward node visits)\n",
    "\n",
    "    Returns: Saves trajectories as a pickle file, 'rewMICE_first_Rvisit.p' in the specified directory, 'real_traj_dir'\n",
    "             If traj_type is 'first_drink', also saves the number of unrewarded water port visits as 'nonRew_RVisits.p'\n",
    "    Return type: TrajS, ndarray[(N, TrajNo, TrajSize), int]\n",
    "                 nonRew_RVisits, ndarray[(N, TrajNo), int]\n",
    "    '''\n",
    "   \n",
    "    N = 10\n",
    "    TrajSize = 1000\n",
    "    TrajNo = 350\n",
    "    TrajS = np.ones((N,TrajNo,TrajSize)) * InvalidState\n",
    "\n",
    "    if traj_type == 'first_Rvisit':\n",
    "        # Extracting trajectories of each rewarded mouse up until the first visit to the reward node\n",
    "        for mouseID, nickname in enumerate(RewNames):\n",
    "            tf = LoadTraj(nickname+'-tf')\n",
    "            reward_found = False\n",
    "            for boutID in np.arange(len(tf.no)):\n",
    "                # find the number of steps till the first reward\n",
    "                for step, entry in enumerate(tf.no[boutID]):\n",
    "                    node, frame = entry\n",
    "                    if node==RewardNode:\n",
    "                        TrajS[mouseID,boutID,step] = tf.no[boutID][step,0]\n",
    "                        reward_found = True\n",
    "                        break\n",
    "                    else:\n",
    "                        TrajS[mouseID,boutID,step] = tf.no[boutID][step,0]\n",
    "                if reward_found:\n",
    "                    break\n",
    "        pickle.dump(TrajS,open(real_traj_dir+'rewMICE_first_Rvisit.p','wb'))\n",
    "    elif traj_type == 'post_firstR_every_Rvisit':\n",
    "        # Extracting trajectories of each rewarded mouse up until the first visit to the reward node\n",
    "        for mouseID, nickname in enumerate(RewNames):\n",
    "            tf = LoadTraj(nickname+'-tf')\n",
    "            first_reward = False\n",
    "            ID = -1\n",
    "            for boutID in np.arange(len(tf.no)):\n",
    "                if len(tf.no[boutID]) > 1:\n",
    "                    for step, entry in enumerate(tf.no[boutID]):\n",
    "                        node, frame = entry\n",
    "                        if node==RewardNode:\n",
    "                            if first_reward:\n",
    "                                TrajS[mouseID,ID,step] = tf.no[boutID][step,0]\n",
    "                            first_reward = True\n",
    "                            break\n",
    "                        elif first_reward:\n",
    "                            TrajS[mouseID,ID,step] = tf.no[boutID][step,0]\n",
    "                    if first_reward:\n",
    "                        ID += 1\n",
    "                \n",
    "        pickle.dump(TrajS,open(real_traj_dir+'rewMICE_post_firstR_every_Rvisit.p','wb'))\n",
    "    elif traj_type == 'first_drink':\n",
    "        # Extracting trajectories of each rewarded mouse up until the first drink is obtained at the reward node\n",
    "        nonRew_RVisits = np.zeros((N,TrajNo), dtype=int)\n",
    "        \n",
    "        for mouseID, nickname in enumerate(RewNames):\n",
    "            tf = LoadTraj(nickname+'-tf')\n",
    "            reward_found = False\n",
    "            for boutID, reFrames in enumerate(tf.re):\n",
    "                waterport_visit_frames = tf.no[boutID][np.where(tf.no[boutID][:,0]==116)[0],1]\n",
    "                if len(reFrames) != 0:\n",
    "                    # find the number of steps till the first reward\n",
    "                    for step, entry in enumerate(tf.no[boutID]):\n",
    "                        node, frame = entry\n",
    "                        if node==116:\n",
    "                            wID = np.where(waterport_visit_frames==frame)[0][0]\n",
    "                            if len(waterport_visit_frames)==1 and waterport_visit_frames[wID] <= reFrames[0][0]:\n",
    "                                reFirst = step\n",
    "                                TrajS[mouseID,boutID,0:reFirst+1] = tf.no[boutID][0:reFirst+1,0] \n",
    "                                reward_found = True\n",
    "                                break\n",
    "                            elif waterport_visit_frames[-1]==frame and waterport_visit_frames[wID] <= reFrames[0][0]:\n",
    "                                reFirst = step\n",
    "                                TrajS[mouseID,boutID,0:reFirst+1] = tf.no[boutID][0:reFirst+1,0] \n",
    "                                reward_found = True\n",
    "                                break\n",
    "                            elif waterport_visit_frames[wID] <= reFrames[0][0] and reFrames[0][0] <= waterport_visit_frames[wID+1]:\n",
    "                                reFirst = step\n",
    "                                TrajS[mouseID,boutID,0:reFirst+1] = tf.no[boutID][0:reFirst+1,0] \n",
    "                                reward_found = True\n",
    "                                break\n",
    "                        else:\n",
    "                            TrajS[mouseID,boutID,step] = tf.no[boutID][step,0]\n",
    "\n",
    "                else:\n",
    "                    TrajS[mouseID,boutID,0:len(tf.no[boutID][:,0])] = tf.no[boutID][:,0]\n",
    "\n",
    "                if reward_found:\n",
    "                    break\n",
    "                    \n",
    "            # Save number of unsuccessful reward node visits\n",
    "            for boutID in np.arange(TrajNo):\n",
    "                nonRew_RVisits[mouseID,boutID] = len(np.where(TrajS[mouseID,boutID,:]==RewardNode)[0])\n",
    "\n",
    "                # Checking if the bout is rewarded\n",
    "                if TrajS[mouseID,boutID+1,0] == InvalidState:\n",
    "                    nonRew_RVisits[mouseID,boutID] -= 1\n",
    "                    break\n",
    "        pickle.dump(nonRew_RVisits,open(real_traj_dir+'nonRew_RVisits.p','wb'))\n",
    "        pickle.dump(TrajS,open(real_traj_dir+'rewMICE_first_drink.p','wb'))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rew_traj('post_firstR_every_Rvisit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_SAnodemap(S,A):\n",
    "    '''\n",
    "    Creates a mapping based on the maze layout where current states are linked to the next 3 future states\n",
    "    \n",
    "    Returns: SAnodemap, a 2D array of current state to future state mappings\n",
    "             Also saves SAnodemap in the specified 'main_dir' as 'nodemap.p'\n",
    "    Return type: ndarray[(S, A), int]\n",
    "    '''\n",
    "    \n",
    "    # Return nodemap for state-action values\n",
    "    SAnodemap = np.ones((S,A), dtype=int) * InvalidState\n",
    "    for node in np.arange(S-1):\n",
    "        # Shallow level node available from current node\n",
    "        if node%2 == 0:\n",
    "            SAnodemap[node,0] = (node - 2) / 2\n",
    "        elif node%2 == 1:\n",
    "            SAnodemap[node,0] = (node - 1) / 2\n",
    "        if SAnodemap[node,0] == InvalidState:\n",
    "            SAnodemap[node,0] = HomeNode\n",
    "\n",
    "        if node not in lv6_nodes:\n",
    "            # Deeper level nodes available from current node\n",
    "            SAnodemap[node,1] = node*2 + 1\n",
    "            SAnodemap[node,2] = node*2 + 2\n",
    "\n",
    "    # Nodes available from entry point\n",
    "    SAnodemap[HomeNode,0] = InvalidState\n",
    "    SAnodemap[HomeNode,1] = 0\n",
    "    SAnodemap[HomeNode,2] = InvalidState\n",
    "    \n",
    "    pickle.dump(SAnodemap,open(main_dir+'nodemap.p','wb'))\n",
    "    \n",
    "    return SAnodemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_trajectory(state_hist_all, episode, save_dir=None, mouse=None, figtitle=None):\n",
    "    '''\n",
    "    Plots specified simulated trajectories on the maze layout.\n",
    "    \n",
    "    state_hist_all: dictionary of trajectories simulated by a model. Eg. state_hist_all{0:[0,1,3..], 1:[]..}\n",
    "    episode: 'all', to plot all trajectories in state_hist_all\n",
    "             int, to plot a specific bout/episode\n",
    "    \n",
    "    Returns: One maze figure with plotted trajectories and a color bar indicating nodes from entry to exit\n",
    "    Return type: --\n",
    "    '''\n",
    "\n",
    "    def nodes2cell(state_hist_all):\n",
    "        '''\n",
    "        simulated trajectories, state_hist_all: {mouseID: [[TrajID x TrajSize]]}\n",
    "        '''\n",
    "        state_hist_cell = []\n",
    "        state_hist_xy = {}\n",
    "        ma=NewMaze(6)\n",
    "        for epID, episode in enumerate(state_hist_all.values()):\n",
    "            cells = []\n",
    "            cells.extend([7])\n",
    "            for id,node in enumerate(episode):\n",
    "                if id != 0 and node != HomeNode:\n",
    "                    if node > episode[id-1]: \n",
    "                        # if going to a deeper node\n",
    "                        cells.extend(ma.ru[node])\n",
    "                    elif node < episode[id-1]: \n",
    "                        # if going to a shallower node\n",
    "                        reverse_path = list(reversed(ma.ru[episode[id-1]]))\n",
    "                        reverse_path = reverse_path + [ma.ru[node][-1]]\n",
    "                        cells.extend(reverse_path[1:])\n",
    "            if node==HomeNode:\n",
    "                home_path = list(reversed(ma.ru[0]))\n",
    "                cells.extend(home_path[1:])  # cells from node 0 to maze exit\n",
    "            state_hist_cell.append(cells)\n",
    "            state_hist_xy[epID] = np.zeros((len(cells),2))\n",
    "            state_hist_xy[epID][:,0] = ma.xc[cells] + np.random.choice([-1,1],len(ma.xc[cells]),p=[0.5,0.5])*np.random.rand(len(ma.xc[cells]))/2\n",
    "            state_hist_xy[epID][:,1] = ma.yc[cells] + np.random.choice([-1,1],len(ma.yc[cells]),p=[0.5,0.5])*np.random.rand(len(ma.yc[cells]))/2\n",
    "        return state_hist_cell, state_hist_xy\n",
    "    \n",
    "    state_hist_cell, state_hist_xy = nodes2cell(state_hist_all)\n",
    "    \n",
    "    ma=NewMaze(6)\n",
    "    # Draw the maze outline    \n",
    "    fig,ax=plt.subplots(figsize=(9,9))\n",
    "    plot(ma.wa[:,0],ma.wa[:,1],fmts=['k-'],equal=True,linewidth=2,yflip=True,\n",
    "              xhide=True,yhide=True,axes=ax)\n",
    "    re=[[-0.5,0.5,1,1],[-0.5,4.5,1,1],[-0.5,8.5,1,1],[-0.5,12.5,1,1],\n",
    "       [2.5,13.5,1,1],[6.5,13.5,1,1],[10.5,13.5,1,1],\n",
    "       [13.5,12.5,1,1],[13.5,8.5,1,1],[13.5,4.5,1,1],[13.5,0.5,1,1],\n",
    "       [10.5,-0.5,1,1],[6.5,-0.5,1,1],[2.5,-0.5,1,1],\n",
    "       [6.5,1.5,1,1],[6.5,11.5,1,1],[10.5,5.5,1,1],[10.5,7.5,1,1],\n",
    "       [5.5,4.5,1,1],[5.5,8.5,1,1],[7.5,4.5,1,1],[7.5,8.5,1,1],[2.5,5.5,1,1],[2.5,7.5,1,1],\n",
    "       [-0.5,2.5,3,1],[-0.5,10.5,3,1],[11.5,10.5,3,1],[11.5,2.5,3,1],[5.5,0.5,3,1],[5.5,12.5,3,1],\n",
    "       [7.5,6.5,7,1]]\n",
    "    for r in re:\n",
    "        rect=patches.Rectangle((r[0],r[1]),r[2],r[3],linewidth=1,edgecolor='lightgray',facecolor='lightgray')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    #plt.axis('off'); # turn off the axes\n",
    "\n",
    "    # Converting cell positions to x,y positions in the maze\n",
    "    # ma.ce contains x,y positions for each cell\n",
    "    if episode == 'all':\n",
    "        for id, episode in enumerate(state_hist_xy):\n",
    "            x = state_hist_xy[episode][:,0]\n",
    "            y = state_hist_xy[episode][:,1]\n",
    "            t = np.linspace(0,1,x.shape[0]) # your \"time\" variable\n",
    "\n",
    "            # set up a list of (x,y) points\n",
    "            points = np.array([x,y]).transpose().reshape(-1,1,2)\n",
    "\n",
    "            # set up a list of segments\n",
    "            segs = np.concatenate([points[:-1],points[1:]],axis=1)\n",
    "\n",
    "            # make the collection of segments\n",
    "            lc = LineCollection(segs, cmap=plt.get_cmap('viridis'),linewidths=2) # jet, viridis hot\n",
    "            lc.set_array(t) # color the segments by our parameter\n",
    "\n",
    "            # plot the collection\n",
    "            lines=ax.add_collection(lc); # add the collection to the plot\n",
    "    else:\n",
    "        x = state_hist_xy[episode][:,0]\n",
    "        y = state_hist_xy[episode][:,1]\n",
    "        t = np.linspace(0,1,x.shape[0]) # your \"time\" variable\n",
    "\n",
    "        # set up a list of (x,y) points\n",
    "        points = np.array([x,y]).transpose().reshape(-1,1,2)\n",
    "\n",
    "        # set up a list of segments\n",
    "        segs = np.concatenate([points[:-1],points[1:]],axis=1)\n",
    "\n",
    "        # make the collection of segments\n",
    "        lc = LineCollection(segs, cmap=plt.get_cmap('viridis'),linewidths=2) # jet, viridis hot\n",
    "        lc.set_array(t) # color the segments by our parameter\n",
    "\n",
    "        # plot the collection\n",
    "        lines=ax.add_collection(lc); # add the collection to the plot\n",
    "\n",
    "    cax=fig.add_axes([1.05, 0.05, 0.05, 0.9])\n",
    "    cbar=fig.colorbar(lines,cax=cax)\n",
    "    cbar.set_ticks([0,1])\n",
    "    cbar.set_ticklabels(['Entry','Exit'])\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    fig.suptitle(figtitle)\n",
    "    fig = plt.gcf()\n",
    "    if save_dir:\n",
    "        fig.savefig(save_dir+mouse+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_pred(avg_V, state_hist_all, save_dir=None):\n",
    "    '''\n",
    "    avg_V: vector of state values averaged across multiple runs of the model\n",
    "           ndarray[(1,S), float]\n",
    "    state_hist_all: dictionary of trajectories simulated by a model. Eg. state_hist_all{0:[0,1,3..], 1:[]..}\n",
    "    \n",
    "    Returns: A heatmap of state values and predicted trajectories plotted on the maze layout\n",
    "    '''\n",
    "    # Plotting state values\n",
    "    fig, ax = plt.subplots(figsize=(30,800))\n",
    "    axhandle = ax.imshow(np.transpose(np.reshape(avg_V,(S,1))),cmap='YlGnBu')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylabel('V (s)')\n",
    "    ax.set_xticks(np.arange(0,127,5))\n",
    "    ax.set_xticklabels([str(val) for val in np.arange(0,127,5)])\n",
    "    ax.set_xlabel('Nodes')\n",
    "    ax.set_title('Average state values for TD(0) with alpha: %.2f, beta: %.2f and gamma: %.2f' %(alpha,beta,gamma))\n",
    "    fig.colorbar(axhandle,fraction=0.005)\n",
    "\n",
    "    print('Max state value ', np.max(avg_V))\n",
    "    print('Min state value', np.min(avg_V))\n",
    "\n",
    "    # Plot predicted trajectories\n",
    "    plot_trajectory(state_hist_all, 'all', save_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def TD0_first_Rvisit(sub_fits,fit_group,fit_group_data):\n",
    "    '''\n",
    "    Predicts trajectories with first reward visit TD(0) using the parameters fitted for each rewarded mouse.\n",
    "    Predicted trajectories can't be longer than its corresponding bout in real mouse trajectories.\n",
    "    Set MatchEndNode = True to set an additional constraint on generating predicted trajectories with the same end node as the real counterpart\n",
    "\n",
    "    Note: use this to generate predicted trajectories from a range of parameters for parameter recovery \n",
    "          or to generate predicted trajectories from fitted parameters\n",
    "    \n",
    "    sub_fits: dictionary of fitted parameters and log likelihood for each rewarded mouse. \n",
    "                   sub_fits{0:[alpha_fit, beta_fit, gamma_fit, LL], 1:[]...., 9:[]}\n",
    "    fit_group: 'Rew' or 'Unrew', specify mouse group to load real trajectory data for\n",
    "    fit_group_data: str, file path for real trajectory data\n",
    "                   \n",
    "    Returns: state_hist_AllMice, dictionary of trajectories simulated by a model using fitted parameters for all Rew mice\n",
    "             state_hist_AllMice{0:[0,1,3..], 1:[]..}\n",
    "             \n",
    "             int valid_bouts, counter to record the number of bouts that were simulated corresponding to real trajectory\n",
    "                              data used for fitting\n",
    "                              \n",
    "             int success, either 0 or 1 to flag when the model fails to generate simulated trajectories adhering\n",
    "                          to certain bounds: fitted parameters, number of episodes, trajectory length\n",
    "    '''\n",
    "    # Set environment parameters\n",
    "    state_hist_AllMice = {}\n",
    "    valid_bouts = []\n",
    "    episode_cap = 500\n",
    "    value_cap = 1e5\n",
    "    success = 1\n",
    "    MatchEndNode = False\n",
    "    \n",
    "    if fit_group == 'Rew':\n",
    "        TrajS = pickle.load(open(fit_group_data,'rb')).astype(int)\n",
    "        \n",
    "    for mouseID in np.arange(10):\n",
    "        # Set model parameters\n",
    "        alpha, beta, gamma, lamda = sub_fits[mouseID]\n",
    "        TrajNo = len(np.where(TrajS[mouseID, :, 0] != InvalidState)[0])\n",
    "        \n",
    "        for count in np.arange(avg_count):\n",
    "            # Initialize model parameters\n",
    "            V = np.zeros(S)  # state-action values\n",
    "            V[HomeNode] = 0  # setting action-values of maze entry to 0\n",
    "            V[RewardNode] = 0  # setting action-values of reward port to 0\n",
    "            state_hist_mouse = {}\n",
    "            R_visits = 0\n",
    "\n",
    "            for n in np.arange(N):\n",
    "                valid_episode = False\n",
    "                episode_attempt = 0\n",
    "                \n",
    "                # Extract from real mouse trajectory the terminal node in current bout and trajectory length\n",
    "                end = np.where(TrajS[mouseID,valid_boutID[n]]==InvalidState)[0][0]\n",
    "                valid_traj = TrajS[mouseID,valid_boutID[n],0:end]\n",
    "                \n",
    "                # Back-up a copy of state-values to use in case the next episode has to be discarded\n",
    "                V_backup = np.copy(V)\n",
    "                \n",
    "                # Begin episode\n",
    "                while not valid_episode and episode_attempt < episode_cap:\n",
    "                    # Initialize starting state,s0 to node 0\n",
    "                    s = StartNode\n",
    "                    state_hist = []\n",
    "                    \n",
    "                    while s!=HomeNode and s!=RewardNode:\n",
    "                        # Record current state\n",
    "                        state_hist.extend([s])\n",
    "\n",
    "                        # Use softmax policy to select action, a at current state, s\n",
    "                        betaV = []\n",
    "                        for node in nodemap[s, :]:\n",
    "                            if node == InvalidState:\n",
    "                                betaV.extend([0])\n",
    "                            else:\n",
    "                                betaV.extend([np.exp(beta * V[node])])\n",
    "                        prob = betaV / np.sum(betaV)\n",
    "                        try:\n",
    "                            a = np.random.choice([0, 1, 2], 1, p=prob)[0]\n",
    "                        except:\n",
    "                            print('Error with probabilities. betaV: ', betaV, ' nodes: ', nodemap[s, :], ' state-values: ', V[nodemap[s, :]])\n",
    "\n",
    "                        # Take action, observe reward and next state\n",
    "                        sprime = int(nodemap[s,a])\n",
    "                        if sprime == RewardNode:\n",
    "                            R = RewardNodeMag  # Receive a reward of 1 when transitioning to the reward port\n",
    "                        else:\n",
    "                            R = 0\n",
    "\n",
    "                        # Update action-value of previous state value, V[s]\n",
    "                        V[s] += alpha * (R + gamma*V[sprime] - V[s])\n",
    "                        if np.isnan(V[s]):\n",
    "                            print('Warning invalid state-value: ', s, sprime, V[s], V[sprime], alpha, beta, gamma, R)\n",
    "                        elif np.isinf(V[s]):\n",
    "                            print('Warning infinite state-value: ', V)\n",
    "                        elif V[s]>value_cap:\n",
    "                            #print('Warning state value exceeded upper bound. Might approach infinity')\n",
    "                            V[s] = value_cap\n",
    "                            \n",
    "                        # Shift state values for the next time step\n",
    "                        s = sprime\n",
    "                        \n",
    "                        # Check whether to abort the current episode\n",
    "                        if len(state_hist) > len(valid_traj):\n",
    "                            #print('Trajectory too long. Aborting episode')\n",
    "                            break\n",
    "                    state_hist.extend([s])\n",
    "\n",
    "                    if abort_episode:\n",
    "                        # Don't save predicted trajectory and attempt episode again\n",
    "                        pass\n",
    "                    else:\n",
    "                        if not MatchEndNode:\n",
    "                            valid_episode = True\n",
    "                        elif MatchEndNode:\n",
    "                            # Checking if predicted trajectory meets another minimum requirement\n",
    "                            # Trajectory must end at the same terminal node as the real trajectory bout\n",
    "                            realTerminalNode = valid_traj[-1]\n",
    "                            if s == realTerminalNode:\n",
    "                                state_hist_mouse[mouseID] = state_hist\n",
    "                                valid_episode = True\n",
    "                            else:\n",
    "                                V = np.copy(V_backup)\n",
    "                                episode_attempt += 1\n",
    "                                #print('Invalid episode: Requirements are to end at ', realTerminalNode, ' with length ', len(valid_traj))\n",
    "                                #print('Predicted Trajectory statistics: ends at ', s, ' with length ', len(state_hist))\n",
    "\n",
    "                if episode_attempt >= episode_cap:\n",
    "                    print('Failed to generate episodes for mouse ', mouseID, ' with parameter set: ', alpha, beta, gamma)\n",
    "                    success = 0\n",
    "                    break\n",
    "            state_hist_AllMice[mouseID] = state_hist_mouse\n",
    "            \n",
    "    return state_hist_AllMice, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def TD0_first_drink(sub_fits,fit_group):\n",
    "    '''\n",
    "    Generating simulated data from the model for STAN fitting\n",
    "    '''\n",
    "    # Set environment parameters\n",
    "    S = 127\n",
    "    A = 3\n",
    "    RT = 1\n",
    "    N = 10\n",
    "    nodemap = get_SAnodemap(S,A)  # rows index the current state, columns index 3 available neighboring states\n",
    "    state_hist_AllMice = {}\n",
    "    valid_bouts = []\n",
    "    avg_count = 1\n",
    "    episode_cap = 500\n",
    "    value_cap = 1e5\n",
    "    success = 1\n",
    "    \n",
    "    if fit_group == 'Rew':\n",
    "        TrajS = pickle.load(open(real_traj_dir+'rewMICE.p','rb')).astype(int)\n",
    "        \n",
    "    for mouseID in np.arange(N):\n",
    "        # Set model parameters\n",
    "        alpha = sub_fits[mouseID][0]  # learning rate\n",
    "        beta = sub_fits[mouseID][1]   # softmax exploration - exploitation\n",
    "        gamma = sub_fits[mouseID][2]\n",
    "        R = 0\n",
    "        \n",
    "        # number of episodes to train over which are real bouts beginning at node 0 \n",
    "        # and exploring deeper into the maze, which is > than a trajectory length of 2 (node 0 -> node 127)\n",
    "        valid_boutID = np.where(TrajS[mouseID,:,2]!=InvalidState)[0]\n",
    "        N = len(valid_boutID)\n",
    "        valid_bouts.extend([N])\n",
    "        \n",
    "        for count in np.arange(avg_count):\n",
    "            # Initialize model parameters\n",
    "            V = np.random.rand(S+1)  # state-action values\n",
    "            V[HomeNode] = 0  # setting action-values of maze entry to 0\n",
    "            V[RewardNode] = 0  # setting action-values of reward port to 0\n",
    "            state_hist_mouse = {}\n",
    "            R_visits = 0\n",
    "\n",
    "            for n in np.arange(N):\n",
    "                valid_episode = False\n",
    "                episode_attempt = 0\n",
    "                total_R_visits = len(np.where(TrajS[mouseID,n,:]==RewardNode)[0])\n",
    "                \n",
    "                # Extract from real mouse trajectory the terminal node in current bout and trajectory length\n",
    "                end = np.where(TrajS[mouseID,valid_boutID[n]]==InvalidState)[0][0]\n",
    "                valid_traj = TrajS[mouseID,valid_boutID[n],0:end]\n",
    "                \n",
    "                # Back-up a copy of state-values to use in case the next episode has to be discarded\n",
    "                V_backup = np.copy(V)\n",
    "                \n",
    "                # Begin episode\n",
    "                while not valid_episode and episode_attempt < episode_cap:\n",
    "                    # Initialize starting state,s0 to node 0\n",
    "                    s = 0\n",
    "                    state_hist = []\n",
    "                    \n",
    "                    while s!=HomeNode and (s!=RewardNode or R==0):\n",
    "                        # Record current state\n",
    "                        state_hist.extend([s])\n",
    "\n",
    "                        # Use softmax policy to select action, a at current state, s\n",
    "                        if s in lv6_nodes:\n",
    "                            aprob = [1,0,0]\n",
    "                        else:\n",
    "                            betaV = [np.exp(beta*V[int(val)]) for val in nodemap[s,:]]\n",
    "                            aprob = []\n",
    "                            for atype in np.arange(3):\n",
    "                                if np.isinf(betaV[atype]):\n",
    "                                    aprob.extend([1])\n",
    "                                elif np.isnan(betaV[atype]):\n",
    "                                    aprob.extend([0])\n",
    "                                else:\n",
    "                                    aprob.extend([betaV[atype]/np.nansum(betaV)])\n",
    "                        \n",
    "                        # Check for invalid probabilities\n",
    "                        for i in aprob:\n",
    "                            if np.isnan(i):\n",
    "                                print('Invalid action probabilities ', aprob, betaV, s)\n",
    "                                print(alpha, beta, gamma, mouseID, n)\n",
    "                        if np.sum(aprob) < 0.999:\n",
    "                            print('Invalid action probabilities, failed summing to 1: ', aprob, betaV, s)\n",
    "                        a = np.random.choice([0,1,2],1,p=aprob)[0]\n",
    "\n",
    "                        # Take action, observe reward and next state\n",
    "                        sprime = int(nodemap[s,a])\n",
    "                        if sprime == RewardNode:\n",
    "                            R_visits += 1\n",
    "                            if R_visits >= total_R_visits:\n",
    "                                R = 1  # Receive a reward of 1 when transitioning to the reward port\n",
    "                            else:\n",
    "                                R = 0\n",
    "                        else:\n",
    "                            R = 0\n",
    "\n",
    "                        # Update action-value of previous state value, V[s]\n",
    "                        #V[s] += alpha * (R + gamma*V[sprime] - V[s])\n",
    "                        V[s] += R + gamma*V[sprime] - alpha*V[s]\n",
    "                        if np.isnan(V[s]):\n",
    "                            print('Warning invalid state-value: ', s, sprime, V[s], V[sprime], alpha, beta, gamma, R)\n",
    "                        elif np.isinf(V[s]):\n",
    "                            print('Warning infinite state-value: ', V)\n",
    "                        elif V[s]>value_cap:\n",
    "                            #print('Warning state value exceeded upper bound. Might approach infinity')\n",
    "                            V[s] = value_cap\n",
    "                            \n",
    "                        # Shift state values for the next time step\n",
    "                        s = sprime\n",
    "                        \n",
    "                        # Check whether to abort the current episode\n",
    "                        if len(state_hist) > len(valid_traj):\n",
    "                            #print('Trajectory too long. Aborting episode')\n",
    "                            break\n",
    "                    state_hist.extend([s])\n",
    "\n",
    "                    # Find actual end node for mouse trajectory in the current bout/episode\n",
    "                    if s == valid_traj[-1]:\n",
    "                        #if len(state_hist) < 200:\n",
    "                        if (len(state_hist) > 2) and (len(state_hist) <= len(valid_traj)):\n",
    "                            state_hist_mouse[n] = state_hist\n",
    "                            valid_episode = True\n",
    "                    else:\n",
    "                        R = 0\n",
    "                        V = np.copy(V_backup)\n",
    "                        #print('Rejecting episode of length: ', len(state_hist), ' for mouse ', mouseID, ' bout ', valid_boutID[n], ' traj length ', len(valid_traj))\n",
    "                        episode_attempt += 1\n",
    "                        \n",
    "                if episode_attempt >= episode_cap:\n",
    "                    print('Failed to generate episodes for mouse ', mouseID, ' with parameter set: ', alpha, beta, gamma)\n",
    "                    success = 0\n",
    "                    break\n",
    "            state_hist_AllMice[mouseID] = state_hist_mouse\n",
    "            \n",
    "            #print('Mouse', mouseID, ' max state-value', np.max(V))\n",
    "    return state_hist_AllMice, valid_bouts, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def TDlambda_Rvisit(sub_fits, fit_group, fit_group_data):\n",
    "    '''\n",
    "    Predict trajectories using TD-lambda model. In this version home and reward node are terminal states.\n",
    "    Predicted trajectories can't be longer than its corresponding bout in real mouse trajectories.\n",
    "    Set MatchEndNode = True to set an additional constraint on generating predicted trajectories with the same end node as the real counterpart\n",
    "    '''\n",
    "    N = 10\n",
    "    state_hist_AllMice = {}\n",
    "    episode_cap = 500\n",
    "    value_cap = 1e5\n",
    "    success = 1\n",
    "    MatchEndNode = False\n",
    "\n",
    "    if fit_group == 'Rew':\n",
    "        TrajS = pickle.load(open(fit_group_data, 'rb')).astype(int)\n",
    "\n",
    "    for mouseID in np.arange(N):\n",
    "        # Set model parameters\n",
    "        alpha, beta, gamma, lamda = sub_fits[mouseID]\n",
    "        TrajNo = len(np.where(TrajS[mouseID, :, 0] != InvalidState)[0])\n",
    "\n",
    "        # Initialize model parameters\n",
    "        if init == 'ZERO':\n",
    "            V = np.zeros(S)  # state-action values\n",
    "        V[HomeNode] = 0  # setting action-values of maze entry to 0\n",
    "        V[RewardNode] = 0  # setting action-values of reward port to 0\n",
    "        e = np.zeros(S)  # eligibility trace vector for all states\n",
    "        state_hist_mouse = {}\n",
    "\n",
    "        for bout in np.arange(TrajNo):\n",
    "            valid_episode = False\n",
    "            abort_episode = False\n",
    "            episode_attempt = 0\n",
    "\n",
    "            # Extract from real mouse trajectory the terminal node in current bout and trajectory length\n",
    "            end = np.where(TrajS[mouseID, bout] == InvalidState)[0][0]\n",
    "            valid_traj = TrajS[mouseID, bout, 0:end]\n",
    "\n",
    "            # Back-up a copy of state-values to use in case the next episode has to be discarded\n",
    "            V_backup = np.copy(V)\n",
    "            e_backup = np.copy(e)\n",
    "\n",
    "            # Begin episode\n",
    "            while not valid_episode and episode_attempt < episode_cap:\n",
    "                # Initialize starting state,s0 to node 0\n",
    "                s = StartNode\n",
    "                state_hist = []\n",
    "\n",
    "                while s != HomeNode and s != RewardNode:\n",
    "                    # Record current state\n",
    "                    state_hist.extend([s])\n",
    "\n",
    "                    # Use softmax policy to select action, a at current state, s\n",
    "                    betaV = []\n",
    "                    for node in nodemap[s, :]:\n",
    "                        if node == InvalidState:\n",
    "                            betaV.extend([0])\n",
    "                        else:\n",
    "                            betaV.extend([np.exp(beta * V[node])])\n",
    "                    prob = betaV / np.sum(betaV)\n",
    "                    try:\n",
    "                        a = np.random.choice([0, 1, 2], 1, p=prob)[0]\n",
    "                    except:\n",
    "                        print('Error with probabilities. betaV: ', betaV, ' nodes: ', nodemap[s, :], ' state-values: ', V[nodemap[s, :]])\n",
    "\n",
    "                    # Take action, observe reward and next state\n",
    "                    sprime = nodemap[s, a]\n",
    "                    if sprime == RewardNode:\n",
    "                        R = RewardNodeMag  # Receive a reward of 1 when transitioning to the reward port\n",
    "                    else:\n",
    "                        R = 0\n",
    "\n",
    "                    # Calculate error signal for current state\n",
    "                    td_error = R + gamma * V[sprime] - V[s]\n",
    "                    e[s] += 1\n",
    "\n",
    "                    # Propagate value to all other states\n",
    "                    for node in np.arange(S):\n",
    "                        V[node] += alpha * td_error * e[node]\n",
    "                        e[node] = gamma * lamda * e[node]\n",
    "\n",
    "                    if np.isnan(V[s]):\n",
    "                        print('Warning invalid state-value: ', s, sprime, V[s], V[sprime], sub_fits)\n",
    "                    elif np.isinf(V[s]):\n",
    "                        print('Warning infinite state-value: ', V)\n",
    "                    elif V[s] > value_cap:\n",
    "                        # print('Warning state value exceeded upper bound. Might approach infinity')\n",
    "                        V[s] = value_cap\n",
    "\n",
    "                    # Update future state to current state\n",
    "                    s = sprime\n",
    "\n",
    "                    # Check whether to abort the current episode\n",
    "                    if len(state_hist) > len(valid_traj):\n",
    "                        # print('Trajectory too long. Aborting episode')\n",
    "                        abort_episode = True\n",
    "                        V = np.copy(V_backup)\n",
    "                        e = np.copy(e_backup)\n",
    "                        episode_attempt += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        abort_episode = False\n",
    "                state_hist.extend([s])\n",
    "\n",
    "                if abort_episode:\n",
    "                    # Don't save predicted trajectory and attempt episode again\n",
    "                    pass\n",
    "                else:\n",
    "                    if not MatchEndNode:\n",
    "                        valid_episode = True\n",
    "                    elif MatchEndNode:\n",
    "                        # Checking if predicted trajectory meets another minimum requirement\n",
    "                        # Trajectory must end at the same terminal node as the real trajectory bout\n",
    "                        realTerminalNode = valid_traj[-1]\n",
    "                        if s == realTerminalNode:\n",
    "                            state_hist_mouse[mouseID] = state_hist\n",
    "                            valid_episode = True\n",
    "                        else:\n",
    "                            V = np.copy(V_backup)\n",
    "                            e = np.copy(e_backup)\n",
    "                            episode_attempt += 1\n",
    "                            #print('Invalid episode: Requirements are to end at ', realTerminalNode, ' with length ', len(valid_traj))\n",
    "                            #print('Predicted Trajectory statistics: ends at ', s, ' with length ', len(state_hist))\n",
    "\n",
    "            if episode_attempt >= episode_cap:\n",
    "                print('Failed to generate episodes for mouse ', mouseID, ' with parameter set: ', alpha, beta, gamma)\n",
    "                success = 0\n",
    "                break\n",
    "        state_hist_AllMice[mouseID] = state_hist_mouse\n",
    "\n",
    "    return state_hist_AllMice, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trajectory lengths over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plotting trajectory lengths vs bouts for all mice\n",
    "fig, axs = plt.subplots(2,5, constrained_layout=True, figsize=(20,15))\n",
    "for mouseID, nickname in enumerate(RewNames):\n",
    "    i = mouseID//5\n",
    "    j = mouseID%5\n",
    "    tf = LoadTraj(nickname+'-tf')\n",
    "    traj_lengths = []\n",
    "    for boutID, bout in enumerate(tf.no):\n",
    "        traj_lengths.extend([len(tf.no[boutID])])\n",
    "    axs[i,j].bar(np.arange(1,len(tf.no)+1), traj_lengths)\n",
    "    axs[i,j].set_title(nickname)\n",
    "    axs[1,j].set_xlabel('Bouts')\n",
    "    axs[i,0].set_ylabel('No. of steps')\n",
    "plt.savefig('figures/all_trajectories.png')\n",
    "    \n",
    "fig2, axs2 = plt.subplots(2,5, constrained_layout=True, figsize=(20,15))\n",
    "for mouseID, nickname in enumerate(RewNames):\n",
    "    i = mouseID//5\n",
    "    j = mouseID%5\n",
    "    tf = LoadTraj(nickname+'-tf')\n",
    "    traj_lengths2 = []\n",
    "    rewardID = [] \n",
    "    visitsteps = []\n",
    "    drinkstep = []\n",
    "    first_drink=False\n",
    "    for boutID, bout in enumerate(tf.no[0:30]):\n",
    "        traj_lengths2.extend([len(tf.no[boutID])])\n",
    "        if 116 in bout:\n",
    "            visits = np.where(tf.no[boutID]==116)[0]\n",
    "            rewardID.extend([boutID]*len(visits))\n",
    "            visitsteps.extend(visits)\n",
    "        if len(tf.re[boutID])>0 and not first_drink:\n",
    "            drinkID = boutID\n",
    "            drinkframe = tf.re[boutID][0][0]\n",
    "            for attempt in visits:\n",
    "                if drinkframe > tf.no[boutID][attempt][1]:\n",
    "                    drinkstep = attempt\n",
    "                    break\n",
    "            first_drink=True\n",
    "    \n",
    "    axs2[i,j].bar(np.arange(len(tf.no[0:30])), traj_lengths2, color='#FFFAC8', edgecolor='#FFD500')\n",
    "    axmin, axmax = axs2[i,j].get_ylim()\n",
    "    ypos = [traj_lengths2[val-1] + (axmax-axmin)*0.1 for val in rewardID]\n",
    "    axs2[i,j].plot(rewardID,visitsteps,'b*',label='reward port visit')\n",
    "    axs2[i,j].plot(drinkID,drinkstep,'k*',label='first drink')\n",
    "    axs2[i,j].set_title(nickname)\n",
    "    axs2[1,j].set_xlabel('Bouts')\n",
    "    axs2[i,0].set_ylabel('No. of steps')\n",
    "    axs2[0,0].legend()\n",
    "plt.savefig('figures/first30_trajectories.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Quadrant Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sorting which quarters of the maze a mouse visits during each trajectory\n",
    "tf = LoadTraj('C1'+'-tf')\n",
    "quad_visit = []\n",
    "for bout in np.arange(3,20):\n",
    "    for val in tf.no[bout]:\n",
    "        if val[0] in quad1:\n",
    "            quad_visit.extend([1])\n",
    "        elif val[0] in quad2:\n",
    "            quad_visit.extend([2])\n",
    "        elif val[0] in quad3:\n",
    "            quad_visit.extend([3])\n",
    "        elif val[0] in quad4:\n",
    "            quad_visit.extend([4])\n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.plot(np.arange(1,len(quad_visit)+1),quad_visit,'*')\n",
    "    plt.title('Bout %i' %(bout+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) <a id='#oneStepTD'></a>\n",
    "Online TD-control algorithm which estimates state values, V(s)\n",
    " - States: 128 maze nodes (including home node)\n",
    " - Terminal states: maze entry, 127 and reward port, 116\n",
    " - Rewards: 0 on all states except for 1 on the water port\n",
    " \n",
    " Pseudocode\n",
    "- Softmax action selection policy: $\\pi(a | s_i) = \\frac{e^{\\beta*V(s_{ij})}}{e^{\\beta*V(s_{ij})} + e^{\\beta*V(s_{ij})} + e^{\\beta*V(s_{ij})}}$, where j indexes the 3 neighboring nodes to $s_i$\n",
    "- State-value update: $V(s) \\leftarrow V(s) + \\alpha*(R + \\gamma*V(s') - V(s))$\n",
    "\n",
    " Re-parametrized version where $\\beta = \\beta * \\alpha$ and $V(s) = \\frac{V(s)}{\\alpha}$\n",
    "- original action-value update: $V(s) \\leftarrow (1-\\alpha)V(s) + \\alpha(R + \\gamma * V(s'))$\n",
    "- new action-value update: $V(s) \\leftarrow (1-\\alpha)V(s) + R + \\alpha * \\gamma * V(s')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define TD(0) model\n",
    "'''\n",
    "Toy TD(0) model to simulate trajectories with\n",
    "\n",
    "Returns: state_hist_all, dictionary of trajectories most recently simulated by the model. \n",
    "         If avg_count > 1, state_hist_all represents the latest simulation results\n",
    "         Type: state_hist_all{0:[0,1,3..], 1:[]..}\n",
    "         \n",
    "         avg_V, average state-values resulting from simulation.\n",
    "         Type: ndarray[(1,S), float]\n",
    "'''\n",
    "\n",
    "# Set environment parameters\n",
    "nodemap = get_SAnodemap(S,A)  # rows index the current state, columns index 3 available neighboring states\n",
    "\n",
    "# Set model parameters\n",
    "alpha = 0.01  # learning rate\n",
    "gamma = 1\n",
    "beta = 10  # softmax exploration - exploitation\n",
    "N = 5  # number of episodes to train over\n",
    "speed = 10  # mice speed in nodes per sec\n",
    "timeout = 10  # units of seconds that the reward port times out\n",
    "\n",
    "# Simulation settings\n",
    "avg_count = 50  # Number of times to repeat the simulation. Only state-values will be averaged, avg_V\n",
    "avg_V = np.zeros(S)\n",
    "\n",
    "for count in np.arange(avg_count):\n",
    "    # Initialize model parameters\n",
    "    V = np.zeros(S) #np.random.rand(S)  # state-action values\n",
    "    V[HomeNode] = 0  # setting action-values of maze entry to 0\n",
    "    V[RewardNode] = 0  # setting action-values of reward port to 0\n",
    "    state_hist_all = {}\n",
    "    total_reward = 0\n",
    "    t = timeout\n",
    "\n",
    "    for n in np.arange(N):\n",
    "        # Initialize starting state,s0 to node 0\n",
    "        s = 0\n",
    "        state_hist = []\n",
    "\n",
    "        # Begin episode\n",
    "        while s!=HomeNode: #and s!=RewardNode:\n",
    "            # Record current state\n",
    "            state_hist.extend([s])\n",
    "\n",
    "            # Use softmax policy to select action, a at current state, s\n",
    "            betaV = [np.exp(beta*V[int(val)]) if val >= 0 else 0 for val in nodemap[s,:]]\n",
    "            aprob = []\n",
    "            if s not in lv6_nodes:\n",
    "                for atype in np.arange(3):\n",
    "                    prob = betaV[atype]/np.nansum(betaV)\n",
    "                    if not np.isnan(prob):\n",
    "                        aprob.extend([prob])\n",
    "                    else:\n",
    "                        aprob.extend([1])\n",
    "                a = np.random.choice([0,1,2],1,p=aprob)[0]\n",
    "            elif s in lv6_nodes:\n",
    "                a = 0  # when s is an end node, chose action for lower level node\n",
    "\n",
    "            # Observe reward and next state based on selected action\n",
    "            sprime = int(nodemap[s,a])\n",
    "            if sprime == RewardNode and t >= timeout:\n",
    "                R = RewardNodeMag  # Receive a reward of 1 when transitioning to the reward port\n",
    "                total_reward += 1\n",
    "                t = 0  # Reset timer\n",
    "            else:\n",
    "                R = 0\n",
    "                t += speed  # Increment time from last reward\n",
    "\n",
    "            # Update action-value of previous state value, V[s]\n",
    "            V[s] += alpha * (R + gamma*V[sprime] - V[s])\n",
    "\n",
    "            # Shift state values for the next time step\n",
    "            s = sprime\n",
    "        state_hist.extend([s])\n",
    "        state_hist_all[n] = state_hist\n",
    "\n",
    "    avg_V += deepcopy(V)\n",
    "avg_V /= avg_count\n",
    "\n",
    "print('alpha: ', alpha, ' beta: ', beta, ' gamma: ', gamma)\n",
    "visualize_pred(avg_V, state_hist_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD($\\lambda$)\n",
    "\n",
    " - States: 128 maze nodes (including home node)\n",
    " - Terminal states: maze entry, 127 and reward port, 116\n",
    " - Rewards: 0 on all states except for 1 on the water port\n",
    " \n",
    " Pseudocode\n",
    "- Softmax action selection policy: $\\pi(a | s_i) = \\frac{e^{\\beta*V(s_{ij})}}{e^{\\beta*V(s_{ij})} + e^{\\beta*V(s_{ij})} + e^{\\beta*V(s_{ij})}}$, where j indexes the 3 neighboring nodes to $s_i$\n",
    "- Temporal difference error: $\\delta = R + \\gamma*V(s') - V(s)$\n",
    "- Eligibility trace: $e(s) \\leftarrow e(s) + 1$\n",
    "- State-value update: $V(s) \\leftarrow V(s) + \\alpha*(\\delta)*e(s)$        [all state-values are updated at each step]\n",
    "- Eligibility trace decay: $e(s) \\leftarrow \\gamma*\\lambda*e(s)$           [eligibility traces of all states are decayed at each step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define TD(lambda) model\n",
    "'''\n",
    "Toy TD(lambda) model to predict trajectories\n",
    "\n",
    "Returns: state_hist_all, dictionary of trajectories most recently simulated by the model. \n",
    "         If avg_count > 1, state_hist_all represents the latest simulation results\n",
    "         Type: state_hist_all{0:[0,1,3..], 1:[]..}\n",
    "         \n",
    "         avg_V, average state-values resulting from simulation.\n",
    "         Type: ndarray[(1,S), float]\n",
    "'''\n",
    "\n",
    "# Set environment parameters\n",
    "nodemap = get_SAnodemap(S,A)  # rows index the current state, columns index 3 available neighboring states\n",
    "\n",
    "# Set model parameters\n",
    "alpha = 0.1   # learning rate, 0 < alpha < 1\n",
    "beta = 10      # softmax exploration - exploitation\n",
    "gamma = 1      # degree of discounting future state values, 0 < gamma < 1 \n",
    "lamda = 0.5    # extent of past states to update, 0 < lambda < 1\n",
    "N = 10          # number of episodes to train over\n",
    "#speed = 10  # mice speed in nodes per sec\n",
    "#timeout = 10  # units of seconds that the reward port times out\n",
    "\n",
    "# Simulation settings\n",
    "avg_count = 1  # Number of times to repeat the simulation. Only state-values will be averaged, avg_V\n",
    "avg_V = np.zeros(S)\n",
    "\n",
    "for count in np.arange(avg_count):\n",
    "    # Initialize model parameters\n",
    "    V = np.zeros(S)    # state values\n",
    "    V[HomeNode] = 0    # setting state-values of maze entry to 0\n",
    "    V[RewardNode] = 0  # setting state-values of reward port to 0\n",
    "    e = np.zeros(S)    # eligibility trace vector for all states\n",
    "    state_hist_all = {}\n",
    "    #t = timeout\n",
    "\n",
    "    for n in np.arange(N):\n",
    "        # Initialize starting state,s0 to node 0\n",
    "        s = 0\n",
    "        state_hist = []\n",
    "\n",
    "        # Begin episode\n",
    "        while s!=HomeNode and s!=RewardNode:\n",
    "            # Record current state\n",
    "            state_hist.extend([s])\n",
    "\n",
    "            # Use softmax policy to select action, a at current state, s\n",
    "            betaV = []\n",
    "            for node in nodemap[s,:]:\n",
    "                if node==InvalidState:\n",
    "                    betaV.extend([0])\n",
    "                else:\n",
    "                    betaV.extend([np.exp(beta*V[node])])\n",
    "            prob = betaV/np.sum(betaV)\n",
    "            try:\n",
    "                a = np.random.choice([0,1,2],1,p=prob)[0]\n",
    "            except ValueError:\n",
    "                print('Error with aprob')\n",
    "                print('Current state: ', s, 'Potential future states: ', nodemap[s,:], ' prob: ', prob)\n",
    "\n",
    "            # Observe reward and next state based on selected action\n",
    "            sprime = nodemap[s,a]\n",
    "            if sprime == RewardNode: #and t >= timeout:\n",
    "                R = RewardNodeMag  # Receive a reward of 1 when transitioning to the reward port\n",
    "                #t = 0  # Reset timer\n",
    "            else:\n",
    "                R = 0\n",
    "                #t += speed  # Increment time from last reward\n",
    "\n",
    "            # Calculate error signal for current state\n",
    "            td_error = R + gamma*V[sprime] - V[s]\n",
    "            e[s] += 1\n",
    "            \n",
    "            # Propagate value to all other states\n",
    "            for node in np.arange(S):\n",
    "                V[node] += alpha * td_error * e[node]\n",
    "                e[node] = gamma * lamda * e[node]\n",
    "\n",
    "            # Update future state to current state\n",
    "            s = sprime\n",
    "            \n",
    "        state_hist.extend([s])\n",
    "        state_hist_all[n] = state_hist\n",
    "\n",
    "    avg_V += deepcopy(V)\n",
    "avg_V /= avg_count\n",
    "\n",
    "print('alpha: ', alpha, ' beta: ', beta, ' gamma: ', gamma, 'lambda: ', lamda)\n",
    "visualize_pred(avg_V, state_hist_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Fake Data to test Parameter Recovery for a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating simulated data for a range of parameter values and saving these trajectories in the directory 'pred_traj_dir'\n",
    "\n",
    "# Set variables\n",
    "file_name = 'set5_a0.05_b2_g0.7.p'\n",
    "sim_data = False\n",
    "save_traj = False\n",
    "N = 10\n",
    "TrajSize = 3000\n",
    "TrajNo = 20\n",
    "TD0_type = 'first_visit'  # Set to 'first_visit' or 'multiple_visit' to run a specific TD(0) model\n",
    "\n",
    "if sim_data:\n",
    "    # Simulating data for parameter recovery\n",
    "    da = 0.1\n",
    "    db = 0.1\n",
    "    dg = 0.1\n",
    "    alpha_range = np.arange(0,1+da,da)\n",
    "    beta_range = np.arange(0,1+db,db)\n",
    "    gamma_range = np.arange(0,1+dg,dg)\n",
    "    true_param = {}\n",
    "    set_counter = 0\n",
    "    for gamma in gamma_range:\n",
    "        for beta in beta_range:\n",
    "            for alpha in alpha_range:\n",
    "                print('Now simulating: ', set_counter, alpha, beta, gamma)\n",
    "                true_param[set_counter] = [alpha, beta, gamma]\n",
    "\n",
    "                sub_fits = {}\n",
    "                for mouseID in np.arange(10):\n",
    "                    sub_fits[mouseID] = [alpha,beta,gamma]\n",
    "\n",
    "                if TD0_type == 'first_visit':\n",
    "                    state_hist_AllMice, valid_bouts, success = TD0_first_visit(sub_fits,'Rew')\n",
    "                elif TD0_type == 'multiple_visit':\n",
    "                    state_hist_AllMice, valid_bouts, success = TD0_multiple_visit(sub_fits,'Rew')\n",
    "\n",
    "                simTrajS = np.ones((N,TrajNo,TrajSize), dtype=int) * InvalidState\n",
    "                for mouseID in np.arange(N):\n",
    "                    for boutID in np.arange(len(state_hist_AllMice[mouseID])):\n",
    "                        simTrajS[mouseID,boutID,0:len(state_hist_AllMice[mouseID][boutID])] = state_hist_AllMice[mouseID][boutID]\n",
    "                if success == 0:\n",
    "                    print('Not saving set ', set_counter)\n",
    "                elif success == 1:\n",
    "                    if TD0_type == 'first_visit':\n",
    "                        pickle.dump(simTrajS,open(pred_traj_dir+'full_search_first_visit/set'+str(set_counter)+'.p','wb'))\n",
    "                    elif TD0_type == 'multiple_visit':\n",
    "                        pickle.dump(simTrajS,open(pred_traj_dir+'full_search/set'+str(set_counter)+'.p','wb'))\n",
    "\n",
    "                # Increment counter\n",
    "                set_counter += 1\n",
    "\n",
    "    # Save true parameter sets\n",
    "    if first_visit_TD:\n",
    "        pickle.dump(true_param,open(pred_traj_dir+'full_search_first_visit/true_param.p','wb'))\n",
    "    else:\n",
    "        pickle.dump(true_param,open(pred_traj_dir+'full_search/true_param.p','wb'))\n",
    "        \n",
    "    # Convert simulated trajectory from a dictionary to an array and save\n",
    "    if save_traj:\n",
    "        simTrajS = np.ones((N,TrajNo,TrajSize), dtype=int) * InvalidState\n",
    "        for mouseID in np.arange(N):\n",
    "            for boutID in np.arange(len(state_hist_AllMice[mouseID])):\n",
    "                simTrajS[mouseID,boutID,0:len(state_hist_AllMice[mouseID][boutID])] = state_hist_AllMice[mouseID][boutID]\n",
    "        pickle.dump(simTrajS,open(pred_traj_dir+file_name,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Simulated Data with the Model (running model forward on simulated choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running the TD(0) model forward on any set of simulated fake data to see what state-values evolve.\n",
    "'''\n",
    "Input: needs simulated trajectories of the form, ndarray[(N, TrajNo, TrajSize), int]\n",
    "\n",
    "Usage: checking how the model might respond to certain sets of trajectories\n",
    "       can also test out on real trajectories\n",
    "'''\n",
    "\n",
    "# Set input\n",
    "simdata_path = 'test_search_cl_nonrp_abgtest_smalldata/set0.p'\n",
    "\n",
    "# Set environment parameters\n",
    "stan_simdata_dir = main_dir+'stan/pre_reward_traj/'\n",
    "RT = 1\n",
    "nodemap = get_SAnodemap(S,A)  # rows index the current state, columns index 3 available neighboring states\n",
    "V = np.ones(S)\n",
    "V[HomeNode] = 0  # setting action-values of maze entry to 0\n",
    "V[RewardNode] = 0  # setting action-values of reward port to 0\n",
    "\n",
    "# Load a simulated trajectory\n",
    "TrajS = pickle.load(open(stan_simdata_dir+simdata_path,'rb'))\n",
    "\n",
    "# Create action sequence from trajectories\n",
    "N = np.shape(TrajS)[0]          # setting the number of rewarded mice\n",
    "B = np.shape(TrajS)[1]          # setting the maximum number of bouts until the first reward was sampled\n",
    "BL = np.shape(TrajS)[2]\n",
    "TrajA = np.zeros(np.shape(TrajS), dtype=int)\n",
    "for n in np.arange(N):\n",
    "    for b in np.arange(B):\n",
    "        for bl in np.arange(BL - 1):\n",
    "            if TrajS[n, b, bl + 1] != InvalidState and TrajS[n, b, bl] != HomeNode:\n",
    "                TrajA[n, b, bl] = np.where(nodemap[TrajS[n, b, bl], :] == TrajS[n, b, bl + 1])[0][0] + 1\n",
    "\n",
    "# Set model parameters corresponding to simulated trajectory\n",
    "alpha = 0.2\n",
    "beta = 2\n",
    "gamma = 0.2\n",
    "mouseID = 0\n",
    "boutsize = np.arange(len(TrajS[mouseID]))\n",
    "\n",
    "for boutID in boutsize:\n",
    "    for stepID, node in enumerate(TrajS[mouseID,boutID,:]):\n",
    "        if node != InvalidState and node != HomeNode and node != RewardNode:\n",
    "            s = node  # Retrieve current state\n",
    "\n",
    "            a = TrajA[0,boutID,stepID]  # Retrieve action taken at state s\n",
    "\n",
    "            # Observe reward and next state based on selected action\n",
    "            sprime = int(nodemap[s,a-1])\n",
    "            if sprime == RewardNode:\n",
    "                R = 1  # Receive a reward of 1 when transitioning to the reward port\n",
    "            else:\n",
    "                R = 0\n",
    "\n",
    "            # Update value of current state\n",
    "            V[s] += alpha * (R + gamma*V[sprime] - V[s])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30,800))\n",
    "axhandle = ax.imshow(np.transpose(np.reshape(V,(S,1))),cmap='RdPu')\n",
    "ax.invert_yaxis()\n",
    "ax.set_ylabel('V (s)')\n",
    "ax.set_xticks(np.arange(0,127,5))\n",
    "ax.set_xticklabels([str(val) for val in np.arange(0,127,5)])\n",
    "ax.set_xlabel('Nodes')\n",
    "ax.set_title('Current state values with alpha: %.2f, beta: %.2f and gamma: %.2f' %(alpha,beta,gamma))\n",
    "fig.colorbar(axhandle,fraction=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simulate data with fitted parameters\n",
    "'''\n",
    "Needs: set of best fit parameters in the form, sub_fits{0:[alpha_fit, beta_fit, gamma_fit, LL], 1:[]...., 9:[]}\n",
    "\n",
    "Usage: After getting fitted parameters from model-fitting (like in STAN), use them to run model simulations\n",
    "       and plot resulting trajectories with the fit results\n",
    "'''\n",
    "\n",
    "sub_fits = pickle.load(open(stan_results_dir+'TD0_cl_nonrp_real/sub_fits.p','rb'))\n",
    "fit_group = 'Rew'\n",
    "fit_group_data = stan_data_dir+'real_traj/rewMICE_first_Rvisit.p'\n",
    "state_hist_AllMice,valid_bouts,_ = TD0_first_Rvisit(sub_fits,fit_group,fit_group_data)\n",
    "\n",
    "# Plotting to compare simulated and actual trajectory lengths\n",
    "sim_lengths_all = {}\n",
    "real_lengths_all = {}\n",
    "rand_LL = {}\n",
    "TrajS = pickle.load(open(fit_group_data,'rb'))\n",
    "plt.figure()\n",
    "for mouseID in np.arange(10):\n",
    "    valid_boutID = np.where(TrajS[mouseID,:,2]!=InvalidState)[0]\n",
    "    real_lengths = []\n",
    "    sim_lengths = []\n",
    "    for boutID, bout in enumerate(valid_boutID):\n",
    "        end = np.where(TrajS[mouseID,bout]==InvalidState)[0][0]\n",
    "        valid_traj = TrajS[mouseID,bout,0:end]\n",
    "        random_choices = [val for val in valid_traj if val not in lv6_nodes]\n",
    "        rand_LL[mouseID] = np.log(0.33) * len(random_choices) \n",
    "        real_lengths.extend([len(valid_traj)])\n",
    "        sim_lengths.extend([len(state_hist_AllMice[mouseID][boutID])])\n",
    "    real_lengths_all[mouseID] = real_lengths\n",
    "    sim_lengths_all[mouseID] = sim_lengths\n",
    "        \n",
    "    plt.plot(np.arange(len(real_lengths_all[mouseID])), real_lengths_all[mouseID], 'r*', label='real')\n",
    "    plt.plot(np.arange(len(real_lengths_all[mouseID])), sim_lengths_all[mouseID], 'g*', label='sim')\n",
    "    if mouseID == 0:\n",
    "        plt.legend()\n",
    "plt.xlabel('Number of bouts till first reward')\n",
    "plt.ylabel('Number of decisions/steps in bout')\n",
    "plt.title('Number of steps in simulated trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save plots generated from model fit results\n",
    "save_dir = 'C:/Users/kdilh/Documents/GitHub/MouseMaze/figures/TD0_firstreward/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "for mouseID, nickname in enumerate(RewNames):\n",
    "    state_hist_cell, state_hist_xy = nodes2cell(state_hist_AllMice[mouseID])\n",
    "    figtitle = 'Simulated trajectory for '+nickname+' with '+str(valid_bouts[mouseID]) \\\n",
    "    +' valid bouts \\n alpha: '+str(np.round(sub_fits[mouseID][0],2))+' beta: '+str(np.round(sub_fits[mouseID][1],2)) \\\n",
    "    +' gamma: '+str(np.round(sub_fits[mouseID][2],2)) + ', subject LL - random LL: ' + str(np.round(sub_fits[mouseID][3] - rand_LL[mouseID],2))\n",
    "    plot_trajectory('all', save_dir, nickname, figtitle)  # enter a single episode ID or enter 'all'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
